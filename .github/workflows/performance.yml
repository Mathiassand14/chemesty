name: Performance Monitoring

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly performance benchmarks
    - cron: '0 2 * * 1'  # Every Monday at 2 AM UTC

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for comparison
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.13'
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: 1.8.0
        virtualenvs-create: true
        virtualenvs-in-project: true
    
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-3.13-${{ hashFiles('**/poetry.lock') }}
    
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-interaction --no-root --with dev
    
    - name: Install project
      run: poetry install --no-interaction
    
    - name: Install additional benchmark dependencies
      run: |
        poetry run pip install psutil
    
    - name: Create benchmark results directory
      run: mkdir -p benchmark_results
    
    - name: Download baseline benchmark results
      uses: actions/cache@v3
      with:
        path: benchmark_results/
        key: benchmark-results-${{ github.repository }}-${{ github.ref_name }}
        restore-keys: |
          benchmark-results-${{ github.repository }}-main
          benchmark-results-${{ github.repository }}-
    
    - name: Run performance benchmarks
      run: |
        cd benchmarks
        poetry run python benchmark_runner.py --iterations 30 --results-dir ../benchmark_results
      continue-on-error: true
    
    - name: Compare with baseline (if available)
      id: comparison
      run: |
        cd benchmarks
        if ls ../benchmark_results/benchmark_results_*.json 1> /dev/null 2>&1; then
          echo "Baseline results found, running comparison..."
          poetry run python benchmark_runner.py --iterations 30 --results-dir ../benchmark_results --compare $(ls -t ../benchmark_results/benchmark_results_*.json | head -2 | tail -1) > comparison_output.txt 2>&1 || true
          
          # Check for performance regressions
          if grep -q "Performance Regressions" comparison_output.txt; then
            echo "performance_regression=true" >> $GITHUB_OUTPUT
            echo "REGRESSION_DETAILS<<EOF" >> $GITHUB_OUTPUT
            grep -A 20 "Performance Regressions" comparison_output.txt >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "performance_regression=false" >> $GITHUB_OUTPUT
          fi
          
          cat comparison_output.txt
        else
          echo "No baseline results found, skipping comparison"
          echo "performance_regression=false" >> $GITHUB_OUTPUT
        fi
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark_results/
        retention-days: 90
    
    - name: Comment on PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read comparison output if it exists
          let comment = '## 🚀 Performance Benchmark Results\n\n';
          
          try {
            const comparisonOutput = fs.readFileSync('benchmarks/comparison_output.txt', 'utf8');
            
            if (comparisonOutput.includes('Performance Regressions')) {
              comment += '⚠️ **Performance regressions detected!**\n\n';
              comment += '```\n' + comparisonOutput + '\n```\n\n';
              comment += 'Please review the performance impact of your changes.\n';
            } else if (comparisonOutput.includes('Performance Improvements')) {
              comment += '✅ **Performance improvements detected!**\n\n';
              comment += '```\n' + comparisonOutput + '\n```\n\n';
              comment += 'Great job on improving performance! 🎉\n';
            } else {
              comment += '✅ No significant performance changes detected.\n\n';
              comment += '```\n' + comparisonOutput + '\n```\n';
            }
          } catch (error) {
            comment += 'Benchmarks completed successfully. No baseline available for comparison.\n';
          }
          
          comment += '\n---\n*This comment was automatically generated by the performance monitoring workflow.*';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
    
    - name: Fail if significant performance regression
      if: steps.comparison.outputs.performance_regression == 'true' && github.event_name == 'pull_request'
      run: |
        echo "❌ Significant performance regression detected!"
        echo "${{ steps.comparison.outputs.REGRESSION_DETAILS }}"
        echo ""
        echo "Please review and optimize the performance impact of your changes."
        echo "If this regression is expected, you can override this check by adding '[skip-perf-check]' to your commit message."
        
        # Check if skip flag is present
        if [[ "${{ github.event.head_commit.message }}" == *"[skip-perf-check]"* ]]; then
          echo "Performance check skipped due to [skip-perf-check] flag."
          exit 0
        else
          exit 1
        fi

  benchmark-report:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
        path: benchmark_results/
    
    - name: Generate performance report
      run: |
        echo "# Weekly Performance Report - $(date)" > performance_report.md
        echo "" >> performance_report.md
        echo "## System Information" >> performance_report.md
        echo "- OS: Ubuntu Latest" >> performance_report.md
        echo "- Python: 3.13" >> performance_report.md
        echo "- Date: $(date)" >> performance_report.md
        echo "" >> performance_report.md
        
        # Add latest benchmark results summary
        if ls benchmark_results/benchmark_results_*.json 1> /dev/null 2>&1; then
          latest_result=$(ls -t benchmark_results/benchmark_results_*.json | head -1)
          echo "## Latest Benchmark Results" >> performance_report.md
          echo "" >> performance_report.md
          echo "Results from: $latest_result" >> performance_report.md
          echo "" >> performance_report.md
          echo '```json' >> performance_report.md
          head -50 "$latest_result" >> performance_report.md
          echo '```' >> performance_report.md
        fi
    
    - name: Create Issue with Performance Report
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('performance_report.md', 'utf8');
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Weekly Performance Report - ${new Date().toISOString().split('T')[0]}`,
            body: report,
            labels: ['performance', 'automated', 'weekly-report']
          });